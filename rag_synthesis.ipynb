{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) - Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olleholleh\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a RunnableLambda that takes a string as input,\n",
    "# reverses it, and then repeats the reversed string twice.\n",
    "runnable = RunnableLambda(lambda s: (s[::-1]) * 2)\n",
    "\n",
    "# Invoke the runnable with the string \"hello\"\n",
    "result = runnable.invoke(\"hello\")\n",
    "\n",
    "print(result)  # Output: \"olleholleh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnable Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation using the | operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: HELLO_WORLD\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a sequence of RunnableLambda instances that process a string\n",
    "# Step 1: Remove leading and trailing whitespaces\n",
    "# Step 2: Convert the string to uppercase\n",
    "# Step 3: Replace spaces with underscores\n",
    "# Step 4: Add \"Processed:\" prefix to the string\n",
    "sequence = RunnableLambda(lambda s: s.strip()) | \\\n",
    "           RunnableLambda(lambda s: s.upper()) | \\\n",
    "           RunnableLambda(lambda s: s.replace(\" \", \"_\")) | \\\n",
    "           RunnableLambda(lambda s: f\"Processed: {s}\")    \n",
    "\n",
    "# Invoke the sequence with the input string \"  hello world  \"\n",
    "result = sequence.invoke(\"  hello world  \")\n",
    "\n",
    "print(result)  # Output: \"Processed: HELLO_WORLD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative implementation by directly calling the RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: HELLO_WORLD\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "# Define individual RunnableLambda steps for processing a string\n",
    "strip = RunnableLambda(lambda s: s.strip())             # Step 1: Remove leading and trailing whitespaces\n",
    "uppercase = RunnableLambda(lambda s: s.upper())         # Step 2: Convert the string to uppercase\n",
    "replace = RunnableLambda(lambda s: s.replace(\" \", \"_\")) # Step 3: Replace spaces with underscores\n",
    "prefix = RunnableLambda(lambda s: f\"Processed: {s}\")    # Step 4: Add \"Processed:\" prefix to the string\n",
    "\n",
    "# Create a RunnableSequence that combines all the individual steps\n",
    "sequence = RunnableSequence(strip, uppercase, replace, prefix)\n",
    "\n",
    "# Invoke the sequence with the input string \"  hello world  \"\n",
    "result = sequence.invoke(\"  hello world  \")\n",
    "\n",
    "print(result)  # Output: \"Processed: HELLO_WORLD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnable Parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reverse': 'olleh', 'uppercase': 'HELLO', 'length': 5}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "# Define individual RunnableLambda functions for different string operations\n",
    "reverse = RunnableLambda(lambda s: s[::-1])      # Function to reverse the input string\n",
    "uppercase = RunnableLambda(lambda s: s.upper())  # Function to convert the string to uppercase\n",
    "length = RunnableLambda(lambda s: len(s))        # Function to calculate the length of the string\n",
    "\n",
    "# Create a RunnableParallel with a mapping (dictionary) of operations\n",
    "parallel = RunnableParallel({\n",
    "    \"reverse\": reverse,\n",
    "    \"uppercase\": uppercase,\n",
    "    \"length\": length\n",
    "})\n",
    "\n",
    "# Invoke the parallel sequence with the input string \"hello\"\n",
    "result = parallel.invoke(\"hello\")\n",
    "\n",
    "print(result)  # Output: {'reverse': 'olleh', 'uppercase': 'HELLO', 'length': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnable Passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a RunnablePassthrough that passes the input unchanged\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "# Invoke the passthrough\n",
    "result = passthrough.invoke(\"hello world\")\n",
    "\n",
    "print(result)  # Output: \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "#### Basic Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Pakistan?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a template for the prompt that includes a placeholder for the country\n",
    "template = \"What is the capital of {country}?\"\n",
    "\n",
    "# Create a PromptTemplate instance with the defined template\n",
    "prompt_template = PromptTemplate(template=template)\n",
    "\n",
    "# Format the template by replacing the placeholder with the specified country (\"Pakistan\")\n",
    "prompt = prompt_template.format(country=\"Pakistan\")\n",
    "\n",
    "print(prompt)  # Output: What is the capital of Pakistan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some examples of questions and answers:\n",
      "\n",
      "User: How can I improve my time management skills?\n",
      "AI: Start by prioritizing your tasks using the Eisenhower Matrix to distinguish between what's urgent and important.\n",
      "\n",
      "User: What are some effective strategies for setting goals?\n",
      "AI: Use the SMART criteria: make your goals Specific, Measurable, Achievable, Relevant, and Time-bound.\n",
      "\n",
      "User: How do I stay motivated during long projects?\n",
      "AI: Break down the project into smaller milestones and celebrate each achievement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Define a list of example queries and their corresponding answers\n",
    "examples = [\n",
    "    {\"query\": \"How can I improve my time management skills?\", \n",
    "     \"answer\": \"Start by prioritizing your tasks using the Eisenhower Matrix to distinguish between what's urgent and important.\"},\n",
    "     \n",
    "    {\"query\": \"What are some effective strategies for setting goals?\", \n",
    "     \"answer\": \"Use the SMART criteria: make your goals Specific, Measurable, Achievable, Relevant, and Time-bound.\"}\n",
    "]\n",
    "\n",
    "# Create a PromptTemplate for formatting the examples\n",
    "example_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],  # Define the input variables for the template\n",
    "    template=\"User: {query}\\nAI: {answer}\"  # Template structure for each example\n",
    ")\n",
    "\n",
    "# Create a FewShotPromptTemplate using the examples and example_template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,                                        \n",
    "    example_prompt=example_template,                           \n",
    "    prefix=\"Here are some examples of questions and answers:\",  \n",
    "    suffix=\"User: {query}\\nAI: {answer}\\n\", \n",
    "    input_variables=[\"query\"],              \n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Format the few-shot prompt with a new user query and corresponding answer\n",
    "prompt = few_shot_prompt.format(query=\"How do I stay motivated during long projects?\",\n",
    "                                answer=\"Break down the project into smaller milestones and celebrate each achievement.\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Can you tell me about Jane Austen?', additional_kwargs={}, response_metadata={}), AIMessage(content='Jane Austen is a renowned author known for Pride and Prejudice.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a ChatPromptTemplate from a list of message tuples. \n",
    "# The first element in each tuple represents the speaker (\"human\" or \"ai\"), \n",
    "# and the second element is the message template.\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Can you tell me about {author}?\"),   # Human asks about the author\n",
    "    (\"ai\", \"{author} is a renowned author known for {notable_work}.\")  # AI responds with author details\n",
    "])\n",
    "\n",
    "# Format the chat prompt with specific values for the placeholders\n",
    "# Replaces {author} with \"Jane Austen\" and {notable_work} with \"Pride and Prejudice\"\n",
    "messages = chat_template.format_messages(author=\"Jane Austen\", notable_work=\"Pride and Prejudice\")\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "#### StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine learning is a subset of artificial intelligence that focuses on enabling systems to learn from data patterns and make decisions without being explicitly programmed. It is widely used in applications like recommendation systems, image recognition, and natural language processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define a string output from an AI explaining a concept\n",
    "ai_response = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that focuses on enabling systems to learn from data patterns and make decisions without being explicitly programmed. It is widely used in applications like recommendation systems, image recognition, and natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "# Use the parser to process the AI response\n",
    "parsed_output = parser.invoke(ai_response)\n",
    "\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSONOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John Doe', 'age': 30, 'occupation': 'Software Engineer', 'skills': ['Python', 'Machine Learning', 'Data Analysis']}\n",
      "Name: John Doe\n",
      "Skills: Python, Machine Learning, Data Analysis\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Initialize the JSON parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Simulated AI response in JSON format (string)\n",
    "ai_response = \"\"\"\n",
    "{\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 30,\n",
    "    \"occupation\": \"Software Engineer\",\n",
    "    \"skills\": [\"Python\", \"Machine Learning\", \"Data Analysis\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Parse the JSON response\n",
    "parsed_output = parser.invoke(ai_response)\n",
    "\n",
    "# Print the parsed output as a dictionary\n",
    "print(parsed_output)\n",
    "\n",
    "# Access specific fields from the parsed JSON output\n",
    "print(f\"Name: {parsed_output['name']}\")\n",
    "print(f\"Skills: {', '.join(parsed_output['skills'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion & Retrieval\n",
    "Let's ingest and retrieve data before calling chains for synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion & Retrieval\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load a PDF document and split it into chunks\n",
    "file_path = \"data/Artificial Intelligence.txt\"  # Path of the document to be loaded\n",
    "loader = TextLoader(file_path)                  # Initialize the text loader\n",
    "documents = loader.load()                       # Load the txt document \n",
    "\n",
    "# Initialize the recursive character text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(              \n",
    "    separators=\"\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")   \n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the Hugging Face embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Store embeddings into the vector store\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Retrieve relevant information using similarity search\n",
    "retriever = vector_store.as_retriever() # uses similarity search by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `Groq API key` using the system environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set the environment variable for the Groq API key using the system environment variable\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chains**\n",
    "#### 1. Question/Answering (Q/A) Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Japan is Tokyo. Established in 1869, Tokyo is one of the 47 prefectures of Japan. It is located on the eastern coast of the island of Honshu and is the most populous city in the world. Tokyo is known for its bustling city life, rich history, and cultural landmarks such as the Tokyo Tower, the Tokyo Skytree, and the historic Asakusa district. It is also home to the Japanese government and the Imperial Palace.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define a prompt template for formatting the question\n",
    "prompt_template = \"Q: {question}\\n\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\"])\n",
    "\n",
    "# Instantiate the ChatGroq LLM with the specified model\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Combine the prompt and LLM into a chain where the prompt output is passed to the LLM\n",
    "qa_chain = prompt | llm  # This creates a chain that takes the formatted question and passes it to the model\n",
    "\n",
    "# Invoke the chain with a specific question\n",
    "response = qa_chain.invoke(\"What is the capital of Japan?\")\n",
    "\n",
    "# Print the content of the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Question/Answering (Q/A) Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to intelligence demonstrated by machines. It is often defined as the ability to solve hard problems or the study of how to make computers do things at which, at the moment, people are better. AI agents are software entities that perceive their environment.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the ChatGroq LLM with the specific model\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Define the system prompt that instructs the LLM how to answer questions based on retrieved context\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"  # Placeholder for the retrieved context\n",
    ")\n",
    "\n",
    "# Create a chat prompt template with a system message and human message\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),  # System message contains instructions and context\n",
    "        (\"human\", \"{input}\"),  # Human message includes the user's input question\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a document chain that combines the LLM with the prompt\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Combine the retrieval chain with the question-answering chain\n",
    "# The retrieval chain retrieves relevant documents and feeds them into the question-answering chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Invoke the chain with a question, and the retriever will provide context for the LLM to generate an answer\n",
    "response = retrieval_chain.invoke({\"input\": \"What is Artificial Intelligence?\"})\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, there?\n",
      "AI:    Hello! Yes, I'm here. How can I help you today? If you have any questions about computer programming or software development, feel free to ask. I'll do my best to provide a clear and helpful answer. 😊 \n",
      "\n",
      "Human: What can you help me with today?\n",
      "AI:    I can help you with a variety of topics related to computer programming and software development. Here are a few examples:\n",
      "\n",
      "1. Explaining programming concepts: If you're new to programming or need help understanding a specific concept, I can explain it in simple terms.\n",
      "2. Debugging code: If you're having trouble with a piece of code, I can help you identify and fix issues.\n",
      "3. Providing code examples: If you're looking for a code example to learn from or use in your own projects, I can provide that.\n",
      "4. Offering guidance on best practices: If you're unsure about the best way to approach a programming problem or design a software system, I can offer advice based on my experience.\n",
      "5. Reviewing and improving code: If you have a piece of code that you'd like me to review and provide suggestions for improvement, I'm happy to help.\n",
      "\n",
      "Just let me know what you need help with, and I'll do my best to assist you. 😊 \n",
      "\n",
      "Human: Can you explain the concept of AI in one sentence?\n",
      "AI:    Artificial intelligence (AI) is the ability of a machine or computer program to mimic intelligent human behavior, such as learning, problem-solving, and decision-making, by using algorithms and data. 😊 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Create an external store (a dictionary) for maintaining chat history across multiple sessions\n",
    "store = {}\n",
    "\n",
    "# Function to get or create chat history for a specific session based on session_id\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    # If the session_id doesn't exist in the store, create a new InMemoryChatMessageHistory\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    # Return the chat history associated with the given session_id\n",
    "    return store[session_id]\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Create a chain that can handle message history, linking it with the LLM and session history retrieval\n",
    "chain = RunnableWithMessageHistory(llm, get_session_history)\n",
    "\n",
    "# First conversation with AI in session \"1\"\n",
    "question_1 = \"Hello, there?\"\n",
    "response_1 = chain.invoke(\n",
    "    question_1,\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},  # Specify the session_id for tracking message history\n",
    ")\n",
    "\n",
    "# Second conversation in the same session \"1\"\n",
    "question_2 = \"What can you help me with today?\"\n",
    "response_2 = chain.invoke(\n",
    "    question_2,\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},  # Continue in the same session \"1\"\n",
    ")\n",
    "\n",
    "# Third conversation in the same session \"1\"\n",
    "question_3 = \"Can you explain the concept of AI in one sentence?\"\n",
    "response_3 = chain.invoke(\n",
    "    question_3,\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},  # Continue in session \"1\"\n",
    ")\n",
    "\n",
    "# Print the responses to see the conversation flow\n",
    "print(\"Human:\", question_1)\n",
    "print(\"AI:   \", response_1.content, \"\\n\")\n",
    "\n",
    "print(\"Human:\", question_2)\n",
    "print(\"AI:   \", response_2.content, \"\\n\")\n",
    "\n",
    "print(\"Human:\", question_3)\n",
    "print(\"AI:   \", response_3.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the document about?\n",
      "AI:    The document is about developing artificial intelligence (AI) in accordance with human rights and democratic values, prioritizing user needs over speculative scenarios, and addressing concerns about fulfilling these principles regardless of the source. A ChatGPT search presumably relates to this context, but the specifics are not provided. \n",
      "\n",
      "Human: Summarize its main points?\n",
      "AI:    The main points of the document are:\n",
      "\n",
      "1. AI development should prioritize user needs, human rights, and democratic values.\n",
      "2. AI should not infringe upon human rights, and granting rights to AI systems could undermine their importance.\n",
      "3. The UK is discussing near and far-term AI risks, considering mandatory and voluntary regulation to address these concerns. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the large language model\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "### Contextualize question ###\n",
    "# Define a system prompt to contextualize the user question by making it standalone\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate for contextualizing questions with chat history\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),      \n",
    "        (\"human\", \"{input}\"),                      \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever to handle chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### Answer question ###\n",
    "# Define a system prompt to generate concise answers using retrieved context\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"  # Placeholder for retrieved context\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate for the question-answering chain\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),            \n",
    "        MessagesPlaceholder(\"chat_history\"),   \n",
    "        (\"human\", \"{input}\"),                \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a document chain for handling question answering using the LLM and the QA prompt\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Combine the history-aware retriever and the QA chain to create the retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "# Store for maintaining chat history across multiple sessions\n",
    "store = {}\n",
    "\n",
    "# Function to get or create chat history for a given session\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # If no session history exists for the given session_id, create a new history\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create a conversational retrieval chain that manages chat history statefully\n",
    "conversational_retrieval_chain = RunnableWithMessageHistory(\n",
    "    retrieval_chain,                 \n",
    "    get_session_history,           \n",
    "    input_messages_key=\"input\",     \n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "# First conversation: question about the document\n",
    "question_1 = {\"input\": \"What is the document about?\"}\n",
    "response_1 = conversational_retrieval_chain.invoke(\n",
    "    question_1,\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")[\"answer\"]\n",
    "\n",
    "# Second conversation in the same session: asking to summarize the main points\n",
    "question_2 = {\"input\": \"Summarize its main points?\"}\n",
    "response_2 = conversational_retrieval_chain.invoke(\n",
    "    question_2,\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]\n",
    "\n",
    "# Printing the responses to see the conversational retrieval flow\n",
    "print(\"Human:\", question_1[\"input\"])\n",
    "print(\"AI:   \", response_1, \"\\n\")\n",
    "\n",
    "print(\"Human:\", question_2[\"input\"])\n",
    "print(\"AI:   \", response_2, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example - Q/A Retrieval Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Ingestion**\n",
    "##### Load, Split, Embed, and Store a PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load a PDF document and split it into chunks\n",
    "file_path = \"data/Dale_Carnegie_Golden_Book-Se.pdf\"  # Path of the document to be loaded\n",
    "loader = PyPDFLoader(file_path)     # Initialize the pdf loader\n",
    "documents = loader.load()           # Load the pdf document \n",
    "\n",
    "# Initialize the recursive character text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(              \n",
    "    separators=\"\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")   \n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the Hugging Face embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Store embeddings into the vector store\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Retrieval**\n",
    "##### Retrieve Documents using Vector Store as a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant information using similarity search\n",
    "retriever = vector_store.as_retriever() # uses similarity search by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Synthesis**\n",
    "##### Generate Response using a Q/A retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, a good leader should possess the following qualities:\n",
      "\n",
      "1. Begin with praise and honest appreciation: A good leader should start by acknowledging the strengths and achievements of others.\n",
      "2. Show tolerance and avoid seeking revenge: A good leader should not hold grudges or try to get even with their enemies, instead, they should show understanding and forgiveness.\n",
      "3. Be prepared for ingratitude: A good leader should not expect constant appreciation or recognition, and should be able to handle ingratitude with grace.\n",
      "4. Strive for excellence: A good leader should always aim to do their best in every situation, and continuously work towards improvement.\n",
      "5. Self-analyze and self-criticize: A good leader should have the ability to objectively analyze their own mistakes, and be willing to accept criticism and learn from it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Set the environment variable for the Groq API key using the system environment variable\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the prompt template to structure how the input context and question are fed into the model\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "# Initialize the LLM (Groq)\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Define the chain of operations:\n",
    "# 1. Get the context through the retriever.\n",
    "# 2. Pass the question without any change using RunnablePassthrough.\n",
    "# 3. Format the input (context & question) through PromptTemplate\n",
    "# 4. Process the prompt through an LLM and return a response.\n",
    "# 5. Parse the response as a string output by StrOutputParser()\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} # Step 1: Pass the context and question\n",
    "    | prompt                                                                # Step 2: Format them with the prompt template\n",
    "    | llm                                                                   # Step 3: Feed the prompt into the model (Groq)\n",
    "    | StrOutputParser()                                                     # Step 4: Parse the model's response into a string\n",
    ")\n",
    "\n",
    "# Invoke the chain to answer the question\n",
    "response = chain.invoke(\"What are the qualities of a good leader?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
